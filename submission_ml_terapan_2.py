# -*- coding: utf-8 -*-
"""Submission_ML_Terapan_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mvSnIw1mYk5i01rQ8vjmaNpq6pOy_DAZ

# Sistem Rekomendasi Game

## Library Import
"""

# Library untuk Mengelola data dan Visualisasi
import numpy as np
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from wordcloud import WordCloud

# Library untuk Membagi Data Menjadi Data Latih dan Uji
from sklearn.model_selection import train_test_split

#Library untuk TF-IDF dan Cosine Similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Library untuk Model Deep Learning
import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

"""Penjelasan:Library-library diatas adalah library yang dipakai selama proses dari awal yaitu data loading sampai evaluasi

## Data Loading
"""

df_game  = pd.read_csv('games_metadata_5k.csv')
df_game

df_rating  = pd.read_csv('game_ratings.csv')
df_rating

"""## EDA"""

df_game.info()

df_rating.info()

"""Penjelasan:Dari df_game kita mendapatkan informasi yaitu terdiri dari 10 kolom dan 5000 baris,dengan 2 kolom bertipe numerik yaitu,game_id dan juga rating dan sisanya bertipe data object.Dari jumlah Non-Null Count kita bisa melihat sekilas ada missing value namun akan di cek lebih lanjut pada tahap berikutnya.
Untuk df_rating kita mendapati terdiri dari 3 kolom dan 273669 baris,dengan 2 kolom bertipe numerik(int) dan satu kolom bertipe object.Sekilas pada Non-Null Count tidak ditemukan missing value.
"""

#Mengecek Missing Values
print("Missing Values pada df_game:")
df_game.isna().sum()

#Mengecek Missing Values
print("\n Missing Values pada df_rating:")
df_rating.isna().sum()

"""Penjelasan:Untuk Data game kita bisa melihat bahwa kolom description,genres,platforms,release dan metacritic_url memiliki missing value.Missing value ini nantinya akan dilakukan imputasi  pada tahap data preparation.Untuk data rating kita tisak menemukan missing value."""

#Cek Duplikasi Data
print("Total Duplikat di df_name:")
print(df_game.duplicated().sum())
print("Total Duplikat di df_rating:")
print(df_rating.duplicated().sum())

"""Penjelasan:Tidak ditemukan nilai duplikasi pada data game
Tidak ditemukan nilai duplikasi pada data rating

### EDA Univariate & Multivariate df_game
"""

#Distribusi Rating Game
plt.figure(figsize=(10, 6))
sns.histplot(df_game['rating'], bins=20, kde=True)
plt.title('Distribusi Rating Game')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')

"""Penjelasan:Distribusi dari rating game cenderung normal namun sedikit miring ke kiri dengan pusat distribusi datanya berada di nilai diatas 3,dengan puncak frekuensi sekitar 3.5 sampai 4"""

#Bar Chart 10 Genre Game Paling Populer
genre_counts = df_game['genres'].str.split(', ').explode().value_counts().head(10)

plt.figure(figsize=(12, 6))
genre_counts.plot(kind='barh', color='skyblue')
plt.title('10 Genre Game Paling Populer')
plt.xlabel('Jumlah Game')

"""Penjelasan:Berdasarkan diagram batang diatas kita bisa melihat bahwa 10 genre terpopuler adalah Action sebanyak kurang dari 3000 diikuti dengan genre indie dan yang terakhir adalah Arcade."""

#Membuat Pie Chart untuk Platform
platform_counts = df_game['platforms'].str.split(', ').explode().value_counts().head(10)

plt.figure(figsize=(12, 6))
platform_counts.plot(kind='pie', autopct='%1.1f%%')
plt.title('Distribusi Platform Game')

"""Penjelasan:Diagram diatas memberikan insight bahwa kebanyakan game dapat di platform PC diikuti oleh macos lalu Playstation 4 dan yang paling sedikit adalah android."""

# Function to remove HTML tags
def remove_html_tags(text):
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

cleaned_texts_for_wordcloud = []

# Iterasi melalui setiap deskripsi di kolom 'description'
for desc in df_game['description']:
    if pd.notna(desc): # Pastikan bukan NaN/None
        cleaned_texts_for_wordcloud.append(remove_html_tags(desc))

# Join all the cleaned descriptions into a single string
text = ' '.join(cleaned_texts_for_wordcloud)

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400).generate(text)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Kata Paling Umum dalam Deskripsi Game')
plt.show()

"""Penjelasan:Wordcloud menunjukkan menunjukkan bahwa deskripsi game umumnya menekankan pada pengalaman pemain, dengan kata-kata seperti "player", "play", dan "experience" yang muncul secara dominan. Selain itu, aspek eksplorasi juga menjadi tema penting, terlihat dari seringnya kata "world", "explore", dan "level" digunakan"""

df_game['year'] = pd.to_datetime(df_game['released']).dt.year
plt.figure(figsize=(12, 6))
sns.scatterplot(x='year', y='rating', data=df_game, alpha=0.5)
plt.title('Rating vs. Tahun Rilis')

"""Penjelasan:Kita bisa melihat bahwa jumlah game yang dirilis meningkat secara signifikan dari waktu ke waktu,terutama pada tahun 2010-an.Untuk distribusi rating game dengan rating 4 dan 5 konsisten terus sepanjang waktu namun untuk tidak ditemukan pola antara tahun rilis dengan rating."""

top_games = df_game.sort_values('rating', ascending=False).head(10)[['name', 'rating']]

plt.figure(figsize=(12, 6))
sns.barplot(x='rating', y='name', data=top_games, palette='viridis')
plt.title('10 Game dengan Rating Tertinggi')

"""Penjelasan:Grafik diatas menunjukkan 10 Game dengan Rating tertinggi.

### EDA Univariate & Multivariate df_rating
"""

plt.figure(figsize=(10, 6))
sns.countplot(x='rating', data=df_rating, palette='viridis')
plt.title('Distribusi Rating User')
plt.xlabel('Rating (1-5)')
plt.ylabel('Jumlah')

"""Penjelasan:Berdasarkan Diagram diatas kita bisa melihat bahwa distribusi rating 1-5 seimbang antar satu sama lain."""

user_activity = df_rating['user_id'].value_counts()
plt.figure(figsize=(12, 6))
sns.histplot(user_activity, bins=50, kde=False)
plt.title('Distribusi Jumlah Rating per User')
plt.xlabel('Jumlah Rating Diberikan')
plt.ylabel('Frekuensi User')

"""Penjelasan:Berdasarkan grafik diatas dapat dilihat bahwa Distribusi jumlah rating per user seimbang.Mayoritas user memberi 25-30 rating."""

game_popularity = df_rating['game_id'].value_counts().head(20)
plt.figure(figsize=(12, 6))
game_popularity.plot(kind='barh', color='skyblue')
plt.title('20 Game Paling Banyak Di-rating')
plt.xlabel('Jumlah Rating')

"""Penjelasan:Berdasarkan diagram kita bisa melihat 20  game_id yang memiliki jumlah rating terbanyak.

## Data Preparation

### Data Preparation-CBF
"""

#1.Drop kolom yang tidak dipakai
columns_to_drop = ['platforms', 'released','cover_image', 'game_link', 'metacritic_url','year','description']
games_cbf = df_game.drop(columns=columns_to_drop)

"""Penjelasan: Kami menghapus kolom-kolom seperti platforms, released, cover_image, game_link, metacritic_url, year, dan description karena informasi di dalamnya tidak cukup relevan dan terlalu banyak noise untuk model Content-Based Filtering yang kami terapkan. Langkah ini juga bertujuan untuk menyederhanakan dimensi dataset agar proses analisis lebih efisien dan fokus hanya pada fitur yang diperlukan."""

#2.Imputasi Missing Values
games_cbf['genres'] = games_cbf['genres'].fillna('Unknown')

# Mengecek kembali Missing Values
print("Missing Values in games_cbf:")
print(games_cbf.isnull().sum())

"""Penjelasan : Langkah ini merupakan bagian dari proses data preparation untuk menangani nilai yang hilang (missing values) pada kolom genres. Nilai kosong pada kolom tersebut diisi dengan string 'Unknown' menggunakan metode imputasi sederhana.

Tahapan ini penting dilakukan karena kolom genres merupakan fitur utama dalam pendekatan Content-Based Filtering (CBF). Fitur ini digunakan untuk menghitung kemiripan antar item (dalam hal ini, game) berdasarkan konten atau atribut yang dimiliki. Jika terdapat nilai kosong, proses representasi konten—misalnya menggunakan metode TF-IDF—dapat terganggu dan menghasilkan output yang kurang akurat. Oleh karena itu, memastikan semua data pada kolom genres terisi adalah langkah krusial dalam menyiapkan dataset untuk CBF.
"""

#3.Ubah Genre Jadi List
games_cbf['genre_list'] = games_cbf['genres'].apply(lambda x: [g.strip() for g in x.split(',')])

"""Penjelasan: Langkah ini merupakan bagian dari proses data preparation yang bertujuan untuk mengubah kolom genres dari format string menjadi format list (daftar). Setiap entri genre yang awalnya dipisahkan oleh koma dipecah menjadi elemen-elemen individual dalam list, dan dibersihkan dari spasi yang tidak perlu.

Tahapan ini diperlukan karena dalam metode Content-Based Filtering (CBF), fitur seperti genre akan digunakan untuk membangun representasi konten item (game), misalnya menggunakan metode TF-IDF atau teknik pemrosesan teks lainnya. Dengan format list, proses ekstraksi fitur dan perhitungan kemiripan antar item menjadi lebih terstruktur dan akurat.
"""

games_cbf.head()

#4. Ekstraksi Fitur TF-IDF dari Kolom 'genres'
tfidf = TfidfVectorizer(stop_words='english')
# Melakukan perhitungan idf pada games_cbf 'genres'
tfidf.fit(games_cbf['genres'])
tfidf.get_feature_names_out()

"""Penjelasan : Pada tahap ini dilakukan proses data preparation menggunakan teknik TF-IDF Vectorization dengan menghapus stop words bahasa Inggris. Langkah ini bertujuan untuk mengubah data teks pada kolom genres menjadi representasi numerik yang dapat digunakan dalam analisis kesamaan antar game. Pertama, objek TfidfVectorizer diinisialisasi dengan parameter stop_words='english' untuk menghilangkan kata-kata umum yang tidak relevan. Selanjutnya, fungsi fit() diterapkan pada kolom genres untuk menghitung nilai IDF dari setiap genre yang muncul. Setelah proses ini, fitur-fitur unik dari genre dapat diperoleh menggunakan get_feature_names_out(). Tahapan ini penting karena memungkinkan sistem rekomendasi berbasis content-based filtering menghitung kemiripan antar game berdasarkan genre secara lebih akurat."""

#5. Transformasi Teks 'genres' Menjadi Matriks TF-IDF
tfidf_matrix = tfidf.fit_transform(games_cbf['genres'])
tfidf_matrix.shape

"""Penjelasan : Setelah melakukan proses fitting, tahap selanjutnya adalah mengubah data genre menjadi bentuk matriks menggunakan fungsi fit_transform(). Pada baris ini, objek tfidf diterapkan pada kolom genres untuk menghasilkan TF-IDF matrix, yaitu representasi numerik dari setiap game berdasarkan genre-nya. Setiap baris pada matriks merepresentasikan satu game, dan setiap kolom mewakili satu genre unik yang telah diproses sebelumnya. Hasil dari tfidf_matrix.shape menunjukkan dimensi matriks, yang memberikan informasi jumlah game (baris) dan jumlah fitur genre (kolom). Proses ini merupakan bagian penting dari data preparation karena memungkinkan sistem melakukan perhitungan matematis, seperti mengukur kemiripan antar game dalam sistem rekomendasi berbasis konten."""

#6. Konversi Matriks TF-IDF ke Bentuk Dense (Matriks Penuh)
tfidf_matrix.todense()

"""Penjelasan : Setelah mendapatkan TF-IDF matrix, langkah berikutnya adalah mengubah format sparse matrix menjadi bentuk matriks penuh (dense) menggunakan fungsi todense(). Hal ini dilakukan agar isi dari matriks dapat lebih mudah dibaca, ditelusuri, atau divisualisasikan, terutama saat proses eksplorasi data atau debugging. Dalam format dense, setiap nilai menunjukkan bobot TF-IDF suatu genre terhadap sebuah game: semakin tinggi nilainya, semakin penting genre tersebut bagi game itu."""

#7. Visualisasi Sampel Matriks TF-IDF dalam Bentuk DataFrame
# Membuat DataFrame untuk melihat TF-IDF matrix
# Kolom diisi dengan genre
# Baris diisi dengan nama game

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=games_cbf.name
).sample(22, axis=1).sample(10, axis=0)

"""Penjelasan :Langkah ini bertujuan untuk menampilkan hasil TF-IDF pada sebuah DataFrame. Nilai-nilai TF-IDF dari setiap game diubah ke format dense dan kemudian dikonversi menjadi tabel dengan baris berupa nama game dan kolom berupa genre unik yang telah dihasilkan sebelumnya. Dengan menggunakan sample(), ditampilkan sebagian kecil dari data secara acak, yaitu 22 genre (kolom) dan 10 game (baris), agar lebih mudah diamati. Tahapan ini termasuk dalam proses visualisasi dan eksplorasi data setelah data preparation, yang berguna untuk memastikan bahwa hasil transformasi TF-IDF sudah sesuai dan representatif sebelum digunakan dalam sistem rekomendasi.

### Data Preparation CF
"""

#1. Mengubah Nama kolom
df_rating.rename(columns={'user_id': 'userID', 'game_id': 'gameID'}, inplace=True)

"""Penjelasan Langkah ini bertujuan untuk mengubah nama dari kolom user_id menjadi userID dan game_id menjadi gameID,proses ini bertujuan untuk membuat nama kolom lebih konsisten dan memudahkan untuk proses selanjutnya dan mengindari error akibat kesalahan pemanggilan kolom."""

#2. Encoded UserID
# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df_rating['userID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Penjelasan : Langkah ini bertujuan untuk melakukan encoding terhadap userID agar data bisa diproses dalam bentuk numerik oleh model. Setiap userID diubah menjadi indeks unik, dan disiapkan pula dictionary kebalikannya untuk proses decoding hasil prediksi kembali ke ID pengguna asli. Encoding ini penting karena algoritma pembelajaran mesin tidak dapat bekerja langsung dengan data string/non-numerik."""

#3.Encoded gameID
# Mengubah gameID menjadi list tanpa nilai yang sama
game_ids = df_rating['gameID'].unique().tolist()
print('list gameID: ', game_ids)

# Melakukan proses encoding gameID
game_to_game_encoded = {x: i for i, x in enumerate(game_ids)}

# Melakukan proses encoding angka ke gameID
game_encoded_to_game = {i: x for i, x in enumerate(game_ids)}

"""Penjelasan : Langkah ini sama seperti sebelumnya yaitu melakukan encoding terhadap kolom gameID"""

#4.Mapping
# Mapping userID ke dataframe user
df_rating['user'] = df_rating['userID'].map(user_to_user_encoded)

# Mapping placeID ke dataframe game
df_rating['game'] = df_rating['gameID'].map(game_to_game_encoded)

"""Penjelasan : Melakukan mapping ID(useID,gameID) ke indeks numerik menggunkan hasil encoding sebelumnya,lalu dimasukkan sebagai dataframe kolom baru(user dan game).Tahapan ini penting karena kolom user dan game berisi indeks yang akan di proses deep learning nantinya."""

#5.Mendapatkan jumlah user dan game
# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah game
num_game = len(game_encoded_to_game)
print(num_game)

print('Number of User: {}, Number of Game:{}'.format(
    num_users,num_game
))

"""Penjelasan : Di Tahapan ini,kita menghitung jumlah user dan game berdasarkan hasil encoding sebelumnya,Tahapan ini diperlukan untuk mendefinisikan dimensi input model dan membantu efisiensi memori."""

#6.Konversi Tipe Data
# Mengubah rating menjadi nilai float
df_rating['rating'] = df_rating['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df_rating['rating'])

# Nilai maksimal rating
max_rating = max(df_rating['rating'])

"""Penjelasan : Melakukan Perubahan tipe data kolom rating menjadi float untuk keperluan modelling dan menampilkan nilai maksimun dan minimun dari kolom rating.Tahapan ini dilakukan untuk keperluan saat modeling karena membutuhkan input bertipe float dan max dan min rating digunakan untuk normalisasi rating"""

7. # Mengacak dataset
df = df_rating.sample(frac=1, random_state=42)
df

"""Penjelasan : Data pada df_rating diacak menggunakan fungsi .sample,tetapi dalam pengacakannya diterapkan random_state = 42 untuk memastikan tiap pengacakan konsisten dan reproducible.
Ini penting dilakukan agar model tidak belajar dari pola urutan data dan mengindari bias.
"""

#8.Feature Construction & Normalization (Scaling)
# Membuat variabel x untuk mencocokkan data user dan game menjadi satu value
x = df[['user', 'game']].values
# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""Penjelasan:Tahapan ini merupakan pembuatan fitur dan normalisasi target, Hal ini dilakukan untuk memastikan data memiliki format dan skala yang sesuai untuk dimasukkan ke dalam model deep learning"""

#9.Data Splitting
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""Penjelasan:Tahapan ini merupakan pembagian dataset dengan porsi 80:20,tahapan ini sangat penting untuk melatih model berdasarkan 80% data yang ada dan 20% lagi sebagai validasi untuk mencegah overfitting

## Modeling

### Model Content-Based Filtering
"""

#Menghitung cosine-similarity antar game berdasarkan proses TDF-IF genre sebelumnya
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Penjelasan:Cosine similarity ini akan menghitung seberapa mirip sebuah game dengan game lainnya berdasarkan genre."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=games_cbf['name'], columns=games_cbf['name'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Penjelasan:Membuat dataframe yang menunjukkan kemiripan antara satu game dengan game dengan indeks dan kolomnya berupa nama game tersebut.0 untuk menunjukkan bahwa tidak ada kemiripan dan 1 untuk menunjukkan bahwa ada kemiripan dengan game tersebut."""

def game_recommendations(nama_game, similarity_data=cosine_sim_df, items=games_cbf[['name', 'genres']], k=5):
    """
    Memberikan rekomendasi game berdasarkan kemiripan dari dataframe similarity.

    Parameter:
    ---
    nama_game : str
        Nama game sebagai indeks dalam dataframe kemiripan.

    similarity_data : pd.DataFrame
        DataFrame simetri berisi nilai kesamaan antar game,
        dengan nama game sebagai indeks dan kolom.

    items : pd.DataFrame
        DataFrame yang memuat nama dan fitur lain dari game
        yang digunakan untuk rekomendasi.

    k : int
        Jumlah rekomendasi game yang ingin ditampilkan.

    Return:
    ---
    pd.DataFrame
        DataFrame berisi k game yang paling mirip dengan nama_game.
    """
    # Mendapatkan index dari game-game dengan similarity terbesar (tanpa termasuk game itu sendiri)
    index = similarity_data.loc[:, nama_game].to_numpy().argpartition(range(-1, -k-1, -1))

    # Mengambil nama-nama game yang paling mirip berdasarkan indeks
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Menghapus nama game asal dari daftar rekomendasi jika masih ada
    closest = closest.drop(nama_game, errors='ignore')

    # Menggabungkan dengan item detail untuk mendapatkan informasi tambahan seperti genre
    return pd.DataFrame(closest, columns=['name']).merge(items, on='name').head(k)

game_recommendations('Grand Theft Auto: San Andreas')

"""### Model Collaborative Content"""

import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model

class RecommenderNet(Model):
    def __init__(self, num_users, num_games, embedding_size, dropout_rate, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-4)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.game_embedding = layers.Embedding(
            num_games, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-4)
        )
        self.game_bias = layers.Embedding(num_games, 1)

        self.concat_dropout = layers.Dropout(dropout_rate)
        self.dense1 = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4))
        self.bn1 = layers.BatchNormalization()
        self.dropout1 = layers.Dropout(dropout_rate)

        self.dense2 = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4))
        self.bn2 = layers.BatchNormalization()
        self.dropout2 = layers.Dropout(dropout_rate)

        self.output_layer = layers.Dense(1, activation='sigmoid')

    def call(self, inputs, training=False):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        game_vector = self.game_embedding(inputs[:, 1])
        game_bias = self.game_bias(inputs[:, 1])

        dot_product = tf.reduce_sum(user_vector * game_vector, axis=1, keepdims=True)
        concat = tf.concat([user_vector, game_vector, dot_product, user_bias, game_bias], axis=1)
        x = self.concat_dropout(concat, training=training)

        x = self.dense1(x)
        x = self.bn1(x, training=training)
        x = self.dropout1(x, training=training)

        x = self.dense2(x)
        x = self.bn2(x, training=training)
        x = self.dropout2(x, training=training)

        return self.output_layer(x)

"""Penjelasan:Membuat model dengan rekomendasi berbasis deep learning. Ini dirancang untuk memprediksi seberapa besar kemungkinan seorang pengguna menyukai sebuah game, dengan pendekatan sebagai berikut:


*   Embedding Layer:Model menggunakan dua buah embedding untuk pengguna dan game
*   Interaksi Pengguna-Game:Vektor embedding dari pengguna dan game digunakan untuk menghitung interaksi melalui dot product. Selain itu, ditambahkan juga bias pengguna dan bias game untuk memperkuat representasi.
*   Concatenation & Deep Layers:Hasil dot product, embedding pengguna dan game, serta bias digabungkan (concatenate) menjadi satu vektor. Vektor ini kemudian diproses melalui dua fully connected layer berukuran 64 dan 32 unit, masing-masing menggunakan aktivasi ReLU, batch normalization, dan dropout untuk regularisasi.
*   Output Layer:Layer terakhir adalah dense layer berukuran 1 dengan aktivasi sigmoid




"""

model = RecommenderNet(num_users, num_game, embedding_size=128,dropout_rate=0.5)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Penjelasan : menginisialisasi model RecommenderNet dengan ukuran embedding 128,dan dropout sebesar 0.5 untuk mencegah model mengalami overfit kemudian melakukan kompilasi terhadap model tersebut menggunakan optimizer Adam dengan learning rate 0.0001. Fungsi loss yang dipakai adalah Mean Squared Error (MSE), yang efektif untuk masalah regresi karena memberikan penalti lebih besar pada kesalahan prediksi yang besar. Sebagai metrik evaluasi, digunakan Root Mean Squared Error (RMSE) yang umum untuk menilai akurasi model rekomendasi, karena mengukur rata-rata kesalahan prediksi dalam skala yang sama dengan rating."""

early_stop = tf.keras.callbacks.EarlyStopping(
      monitor='val_loss',
      patience=5,
      min_delta=1e-4,
      restore_best_weights=True,
      verbose=1
  )
lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(
      monitor='val_loss',
      factor=0.5,
      patience=2,
      min_lr=1e-5,
      verbose=1
  )

"""Penjelasan : Ini menginisialisasi dua callback penting untuk proses pelatihan model agar lebih efisien dan mencegah overfitting:

EarlyStopping digunakan untuk menghentikan pelatihan lebih awal ketika nilai validasi loss (val_loss) sudah tidak membaik secara signifikan (min_delta=1e-4) selama 5 epoch berturut-turut (patience=5). Selain itu, model akan mengembalikan bobot terbaik yang pernah didapatkan selama pelatihan (restore_best_weights=True). Callback ini membantu menghindari pelatihan berlebihan (overfitting) dan menghemat waktu.

ReduceLROnPlateau menurunkan laju pembelajaran (learning rate) secara otomatis jika validasi loss tidak membaik selama 2 epoch berturut-turut (patience=2). Penurunan dilakukan dengan faktor 0.5 (factor=0.5), namun laju pembelajaran tidak akan turun di bawah nilai minimum yang sudah ditetapkan (min_lr=1e-5). Hal ini memungkinkan optimizer untuk melakukan langkah pembaruan bobot yang lebih halus saat pelatihan mulai mendekati konvergensi, sehingga model bisa mencapai performa lebih baik.
"""

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=256,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[early_stop,lr_reduce],
    verbose=1
)

"""Penjelasan:Proses pelatihan model dilakukan menggunakan fungsi model.fit dengan data pelatihan x_train dan y_train, serta data validasi x_val dan y_val untuk mengevaluasi kinerja model di setiap epoch. Pelatihan dijalankan selama maksimal 50 epoch dengan ukuran batch sebesar 256 untuk memastikan efisiensi dalam proses update bobot. Selama pelatihan, digunakan dua callback, yaitu EarlyStopping dan ReduceLROnPlateau, untuk mengendalikan proses pelatihan secara adaptif. EarlyStopping akan menghentikan pelatihan lebih awal apabila validasi loss tidak menunjukkan perbaikan selama 5 epoch berturut-turut, serta mengembalikan bobot model terbaik. Sementara itu, ReduceLROnPlateau akan menurunkan learning rate sebesar 0.5 jika validasi loss tidak membaik selama 2 epoch, dengan tujuan agar model tetap bisa belajar secara halus saat mendekati konvergensi. Parameter verbose=1 digunakan agar proses pelatihan ditampilkan secara ringkas di setiap epoch, sehingga perkembangan performa model dapat dipantau secara langsung.

Penjelasan:Dilihat dari 20 epoch yang berjalan nilai Train RMSE nya terus menurun,tetapi di beberapa epoch terakhir untuk val_RMSE terjadi peningkatan,namun tidak terlalu jauh dengan nilai trainnya hanya sekitar 0.0286 dan juga callback dari EarlyStopping dan ReduceLROnPlateau bekerja dengan baik untuk menghindari overfitting.
Sehingga dikembalikan nilai model yang terbaik berada pada epoch ke 15
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_RMSE')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Penjelasan:Membuat visualisasi RMSE antara data train dengan data validation."""

# Ambil 1 user secara acak
user_id = df.userID.sample(1).iloc[0]
game_played_by_user = df[df.userID == user_id]

# Game yang belum dimainkan user
game_not_played = df_game[~df_game['game_id'].isin(game_played_by_user.gameID.values)]['game_id']
game_not_played = list(set(game_not_played).intersection(set(game_to_game_encoded.keys())))

# Encoding game dan user
game_not_played = [[game_to_game_encoded.get(x)] for x in game_not_played]
user_encoder = user_to_user_encoded.get(user_id)
user_game_array = np.hstack(
    ([[user_encoder]] * len(game_not_played), game_not_played)
)

# Prediksi rating user untuk game yang belum dimainkan
ratings = model.predict(user_game_array).flatten()

# Ambil top 10 prediksi rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_game_ids = [
    game_encoded_to_game.get(game_not_played[x][0]) for x in top_ratings_indices
]

# Tampilkan hasil
print('Rekomendasi untuk user:', user_id)
print('===' * 9)

print('Game dengan rating tertinggi oleh user')
print('----' * 8)
top_game_user = (
    game_played_by_user.sort_values(by='rating', ascending=False)
    .head(5)
    .gameID.values
)
top_game_rows = df_game[df_game['game_id'].isin(top_game_user)]
for row in top_game_rows.itertuples():
    print(row.name, ':', row.genres)

print('----' * 8)
print('Top 10 Rekomendasi Game')
print('----' * 8)
recommended_games = df_game[df_game['game_id'].isin(recommended_game_ids)]
for row in recommended_games.itertuples():
    print(row.name, ':', row.genres)

"""## Evaluasi

### Evaluasi Content-Based Filtering

**Metrik Evaluasi Content-Based Filtering: Presisi**

Presisi adalah metrik yang digunakan untuk mengukur seberapa relevan item yang direkomendasikan oleh sistem. Ini dihitung sebagai rasio jumlah rekomendasi yang relevan dengan total jumlah item yang direkomendasikan.


$$
Precision@k = \frac{\text{Jumlah Item relevan dalam top-k}}{\text{k}}
$$

Dimana:


*   Jumlah item relevan dalam top-k berarti: dari top-k rekomendasi, berapa banyak item yang juga muncul dalam ground truth.
*   k adalah jumlah rekomendasi yang dievaluasi



Bagaimana Precision@k bekerja:

1.Model akan memberikan hasil rekomendasi kepada user

2.Ambil top-k dari hasil rekomendasi tersebut

3.Bandingkan dengan ground truth (daftar item relevan untuk user)

4.Hitung berapa banyak dari top-k item yang juga ada di ground truth

5.Hitung Precision@k

**Implementasi Precision@k dan Evaluasi Model**
"""

# Ground truth: daftar game relevan
relevant_games = [
    'Grand Theft Auto V',
    'Grand Theft Auto IV',
    'Red Dead Redemption',
    'Ghostbusters: The Video Game',
    'Silent Hill 4: The Room'
]

"""Penjelasan:Kita memiliki Ground Truth sebagai berikut."""

# Mendapatkan rekomendasi berdasarkan game 'Grand Theft Auto: San Andreas'
recommended_games = game_recommendations('Grand Theft Auto: San Andreas', k=5)['name'].tolist()

print("Rekomendasi:")
for game in recommended_games:
    print("-", game)

"""Penjelasan:Berdasarkan rekomendasi game untuk Grand Theft Auto: San Andreas; untuk k = 5,maka didapat Jumlah item relevan dalam top-k = 3"""

def precision_at_k(recommended, relevant, k=5):
    recommended_top_k = recommended[:k]
    relevant_set = set(relevant)
    hits = sum(1 for game in recommended_top_k if game in relevant_set)
    return hits / k

# Hitung Precision@5
p_at_5 = precision_at_k(recommended_games, relevant_games, k=5)

print(f"\nPrecision@5: {p_at_5:.2f}")

"""Hasil Evaluasi : Berdasarkan Jumlah item relevan dalam top-k = 3 dan k = jumlah rekomendasi yang di evaluasi  = 5 maka didapat hasil Precision@k  = 3/5 = 0.60.
Berdasarkan hasil evaluasi, diperoleh nilai Precision@k sebesar 0.60, yang menunjukkan bahwa 60% dari item yang direkomendasikan oleh sistem dalam top-5 adalah relevan bagi pengguna. Hal ini mengindikasikan bahwa sistem memiliki tingkat ketepatan yang cukup baik dalam memberikan rekomendasi yang sesuai dengan preferensi pengguna.

### Evaluasi Collaborative Filtering

**Metrik Evaluasi Collaborative Filtering: MSE & RMSE**

#### Mean Squared Error (MSE)
MAE mengukur rata-rata selisih absolut antara nilai prediksi dan nilai aktual.

**Rumus MSE:**

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Dimana:

- $n$ = jumlah sampel (data prediksi)

- $y_i$ = rating aktual ke-i

- $\hat{y}_i$ = rating hasil prediksi ke-i


#### Root Mean Squared Error (RMSE)
RMSE mengukur akar dari rata-rata kuadrat selisih antara nilai prediksi dan aktual.

**Rumus RMSE:**

$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} = \sqrt{\text{MSE}}
$$

Dimana:
- $n$ = jumlah sampel (data prediksi)

- $y_i$ = rating aktual ke-i

- $\hat{y}_i$ = rating hasil prediksi ke-i

**Implementasi MSE,RMSE dan Evaluasi Model**
"""

results = model.evaluate(x_val, y_val, verbose=1)
print(f"\n[Hasil Evaluasi terhadap Data Validasi]")
print(f"Loss (MSE): {results[0]:.4f}")
print(f"RMSE      : {results[1]:.4f}")

"""Hasil Evaluasi Collaborative Filtering menunjukkan performa pada data validasi dengan nilai Loss(MSE) sebesar 0.1369 dan RMSE sebesar 0.3569.Nilai MSE tersebut menunjukkan rata-rata kuadrat selisih antara rating aktual dan prediksi cukup kecil, yang berarti model memiliki tingkat kesalahan yang rendah dalam memprediksi rating user terhadap game.Dan Nilai RMSE,artinya secara rata-rata, prediksi rating model hanya meleset sekitar 0.35 poin dari rating asli, yang termasuk cukup baik untuk skala normalized rating (0–1)."""